{
  "name": "deepseek-v3",
  "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
  "pull_count": 1800000,
  "last_updated": "Jan 13, 2025 5:16 PM UTC",
  "page_hash": "03119968ccfe",
  "tags": [
    {
      "name": "application/vnd.ollama.image.model",
      "digest": "sha256:d83c18fb2a2ccca56d641920f01e6fe533dfb3fcf4b77c007c931497cd24a517",
      "size": "376.65GB",
      "media_type": "application/vnd.ollama.image.model",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.template",
      "digest": "sha256:c5ce92dfece191ff732e0a40245acac9a09ad23ab418875c1ba3bb2e8ce6e97d",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.template",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.license",
      "digest": "sha256:3a7c2cf04638420056babe5607700fbe8c80b4b2ef568a5c06ad1836809c77a4",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.license",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.params",
      "digest": "sha256:f4d24e9138dd4603380add165d2b0d970bef471fac194b436ebd50e6147c6588",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.params",
      "annotations": {}
    }
  ]
}