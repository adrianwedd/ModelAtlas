{
  "name": "mixtral",
  "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
  "pull_count": 1100000,
  "last_updated": "Dec 20, 2024 8:27 AM UTC",
  "page_hash": "d2625c2262aa",
  "tags": [
    {
      "name": "application/vnd.ollama.image.model",
      "digest": "sha256:f2dc41fa964b42bfe34e9fb09c0acdcfbfd6e52f1332930b4eacc9d6ad1c6cd2",
      "size": "24.63GB",
      "media_type": "application/vnd.ollama.image.model",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.template",
      "digest": "sha256:53d74de0d84c4e8f2064f2285affe49b22af5cc2ba67978afe7b4b3cb41f9b21",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.template",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.license",
      "digest": "sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.license",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.params",
      "digest": "sha256:ed11eda7790d05b49395598a42b155812b17e263214292f7b87d15e14003d337",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.params",
      "annotations": {}
    }
  ]
}