{
  "name": "llava",
  "description": "\ud83c\udf0b LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
  "pull_count": 7700000,
  "last_updated": "Feb 1, 2024 8:41 AM UTC",
  "page_hash": "238c382096ea",
  "tags": [
    {
      "name": "application/vnd.ollama.image.model",
      "digest": "sha256:170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868",
      "size": "3.83GB",
      "media_type": "application/vnd.ollama.image.model",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.projector",
      "digest": "sha256:72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539",
      "size": "0.58GB",
      "media_type": "application/vnd.ollama.image.projector",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.license",
      "digest": "sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.license",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.template",
      "digest": "sha256:c43332387573e98fdfad4a606171279955b53d891ba2500552c2984a6560ffb4",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.template",
      "annotations": {}
    },
    {
      "name": "application/vnd.ollama.image.params",
      "digest": "sha256:ed11eda7790d05b49395598a42b155812b17e263214292f7b87d15e14003d337",
      "size": "0.0GB",
      "media_type": "application/vnd.ollama.image.params",
      "annotations": {}
    }
  ]
}