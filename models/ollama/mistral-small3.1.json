{
  "name": "mistral-small3.1",
  "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
  "license": "Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.",
  "pull_count": 173700,
  "last_updated": "Apr 7, 2025 4:59 PM UTC",
  "readme_html": "<div class=\"prose-td code:display-inline-block prose-td code:bg-gray-200 prose-td code:px-2 prose-td code:py-1 prose-td code:rounded-md prose prose-headings:mb-[0.7em] prose-headings:mt-[1.25em] prose-headings:font-semibold prose-headings:tracking-tight prose-h1:text-[32px] prose-h2:text-2xl prose-h3:text-xl prose-h4:text-lg prose-h5:text-base prose-p:mb-4 prose-p:mt-0 prose-p:leading-relaxed prose-p:before:hidden prose-p:after:hidden prose-blockquote:font-normal prose-blockquote:not-italic prose-blockquote:text-neutral-500 prose-blockquote:before:hidden prose-blockquote:after:hidden prose-code:my-0 prose-code:inline-block prose-code:rounded-md prose-code:bg-neutral-100 prose-code:px-2 prose-code:text-[85%] prose-code:font-normal prose-code:leading-relaxed prose-code:text-black prose-code:before:hidden prose-code:after:hidden prose-pre:mb-4 prose-pre:mt-0 prose-pre:whitespace-pre-wrap prose-pre:rounded-lg prose-pre:bg-neutral-100 prose-pre:px-3 prose-pre:py-3 prose-pre:text-base prose-pre:text-black prose-ol:mb-4 prose-ol:mt-1 prose-ol:pl-8 marker:prose-ol:text-black prose-ul:mb-4 prose-ul:mt-1 prose-ul:pl-8 marker:prose-ul:text-black prose-li:mb-0 prose-li:mt-0.5 prose-li:text-black first:prose-li:mt-0 prose-table:w-full prose-table:table-auto prose-table:border-collapse prose-th:break-words prose-th:text-center prose-th:font-semibold prose-td:break-words prose-td:px-4 prose-td:py-2 prose-td:text-left prose-img:mx-auto prose-img:my-12 prose-video:my-12 max-w-none overflow-auto py-5 text-black\" id=\"display\">\n<blockquote>\n<p>Note: this model requires Ollama 0.6.5 or higher. <a href=\"https://ollama.com/download\" rel=\"nofollow\">Download Ollama</a></p>\n</blockquote>\n<p><img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" width=\"120\"/></p>\n<p>Building on <a href=\"https://ollama.com/library/mistral-small\" rel=\"nofollow\">Mistral Small 3</a>, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.</p>\n<p>Mistral Small 3.1 is released under an Apache 2.0 license.</p>\n<h2>Key features and capabilities</h2>\n<ul>\n<li><p>Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.</p></li>\n<li><p>Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential.</p></li>\n<li><p>Low-latency function calling: Capable of rapid function execution within automated or agentic workflows</p></li>\n<li><p>Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.</p></li>\n<li><p>Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.</p></li>\n</ul>\n<h2>References</h2>\n<p><a href=\"https://mistral.ai/news/mistral-small-3-1\" rel=\"nofollow\">Blog post</a></p>\n</div>",
  "page_hash": "f49b55f9d861",
  "readme_text": "\n\nNote: this model requires Ollama 0.6.5 or higher. Download Ollama\n\n\nBuilding on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.\nMistral Small 3.1 is released under an Apache 2.0 license.\nKey features and capabilities\n\nLightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.\nFast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential.\nLow-latency function calling: Capable of rapid function execution within automated or agentic workflows\nFine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.\nFoundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.\n\nReferences\nBlog post\n",
  "tags": [
    {
      "tag": "mistral-small3.1:latest",
      "last_updated": "3 months ago",
      "size": "15GB",
      "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
      "manifest": {
        "schemaVersion": 2,
        "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
        "config": {
          "mediaType": "application/vnd.docker.container.image.v1+json",
          "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
          "size": 494
        },
        "layers": [
          {
            "mediaType": "Model Weights",
            "digest": "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "size": 15486896384
          },
          {
            "mediaType": "Template",
            "digest": "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "size": 695
          },
          {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "size": 1526
          },
          {
            "mediaType": "Parameters",
            "digest": "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e",
            "size": 17
          }
        ]
      },
      "config": {
        "model_format": "gguf",
        "model_family": "mistral3",
        "model_families": [
          "mistral3"
        ],
        "model_type": "24.0B",
        "file_type": "Q4_K_M",
        "architecture": "amd64",
        "os": "linux",
        "rootfs": {
          "type": "layers",
          "diff_ids": [
            "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e"
          ]
        }
      },
      "context_length": null,
      "model_type": "24.0B",
      "quantization": null,
      "base_model": null
    },
    {
      "tag": "mistral-small3.1:24b",
      "last_updated": "3 months ago",
      "size": "15GB",
      "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
      "manifest": {
        "schemaVersion": 2,
        "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
        "config": {
          "mediaType": "application/vnd.docker.container.image.v1+json",
          "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
          "size": 494
        },
        "layers": [
          {
            "mediaType": "Model Weights",
            "digest": "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "size": 15486896384
          },
          {
            "mediaType": "Template",
            "digest": "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "size": 695
          },
          {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "size": 1526
          },
          {
            "mediaType": "Parameters",
            "digest": "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e",
            "size": 17
          }
        ]
      },
      "config": {
        "model_format": "gguf",
        "model_family": "mistral3",
        "model_families": [
          "mistral3"
        ],
        "model_type": "24.0B",
        "file_type": "Q4_K_M",
        "architecture": "amd64",
        "os": "linux",
        "rootfs": {
          "type": "layers",
          "diff_ids": [
            "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e"
          ]
        }
      },
      "context_length": null,
      "model_type": "24.0B",
      "quantization": null,
      "base_model": null
    },
    {
      "tag": "mistral-small3.1:24b-instruct-2503-q4_K_M",
      "last_updated": "3 months ago",
      "size": "15GB",
      "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
      "manifest": {
        "schemaVersion": 2,
        "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
        "config": {
          "mediaType": "application/vnd.docker.container.image.v1+json",
          "digest": "sha256:9b6ac0d4e97e3f367b82ed5271ce22f33b7083a14907fdd839673bb85eb356c6",
          "size": 494
        },
        "layers": [
          {
            "mediaType": "Model Weights",
            "digest": "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "size": 15486896384
          },
          {
            "mediaType": "Template",
            "digest": "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "size": 695
          },
          {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "size": 1526
          },
          {
            "mediaType": "Parameters",
            "digest": "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e",
            "size": 17
          }
        ]
      },
      "config": {
        "model_format": "gguf",
        "model_family": "mistral3",
        "model_families": [
          "mistral3"
        ],
        "model_type": "24.0B",
        "file_type": "Q4_K_M",
        "architecture": "amd64",
        "os": "linux",
        "rootfs": {
          "type": "layers",
          "diff_ids": [
            "sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc",
            "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e"
          ]
        }
      },
      "context_length": null,
      "model_type": "24.0B",
      "quantization": null,
      "base_model": null
    },
    {
      "tag": "mistral-small3.1:24b-instruct-2503-q8_0",
      "last_updated": "3 months ago",
      "size": "26GB",
      "digest": "sha256:85ffb6a11300b5bba47ac81cb359a93d8c3d8a40aff23361f6a2c2403389c2e9",
      "manifest": {
        "schemaVersion": 2,
        "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
        "config": {
          "mediaType": "application/vnd.docker.container.image.v1+json",
          "digest": "sha256:85ffb6a11300b5bba47ac81cb359a93d8c3d8a40aff23361f6a2c2403389c2e9",
          "size": 492
        },
        "layers": [
          {
            "mediaType": "Model Weights",
            "digest": "sha256:de0f4b9634e4bb82a84dd0a376c4a6787dbf4ce5b52a62e39be103bc9c8245d0",
            "size": 26129942784
          },
          {
            "mediaType": "Template",
            "digest": "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "size": 695
          },
          {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "size": 1526
          },
          {
            "mediaType": "Parameters",
            "digest": "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e",
            "size": 17
          }
        ]
      },
      "config": {
        "model_format": "gguf",
        "model_family": "mistral3",
        "model_families": [
          "mistral3"
        ],
        "model_type": "24.0B",
        "file_type": "Q8_0",
        "architecture": "amd64",
        "os": "linux",
        "rootfs": {
          "type": "layers",
          "diff_ids": [
            "sha256:de0f4b9634e4bb82a84dd0a376c4a6787dbf4ce5b52a62e39be103bc9c8245d0",
            "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e"
          ]
        }
      },
      "context_length": null,
      "model_type": "24.0B",
      "quantization": null,
      "base_model": null
    },
    {
      "tag": "mistral-small3.1:24b-instruct-2503-fp16",
      "last_updated": "3 months ago",
      "size": "48GB",
      "digest": "sha256:4cffd50b0169c61e8138813dc1081e0295708a7b2033e15e84c62d7f6f4b1864",
      "manifest": {
        "schemaVersion": 2,
        "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
        "config": {
          "mediaType": "application/vnd.docker.container.image.v1+json",
          "digest": "sha256:4cffd50b0169c61e8138813dc1081e0295708a7b2033e15e84c62d7f6f4b1864",
          "size": 491
        },
        "layers": [
          {
            "mediaType": "Model Weights",
            "digest": "sha256:e77b96cf6204c5022350ddda84f65151309ad925db54535dfa8ffaa771131315",
            "size": 48032073984
          },
          {
            "mediaType": "Template",
            "digest": "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "size": 695
          },
          {
            "mediaType": "application/vnd.ollama.image.system",
            "digest": "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "size": 1526
          },
          {
            "mediaType": "Parameters",
            "digest": "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e",
            "size": 17
          }
        ]
      },
      "config": {
        "model_format": "gguf",
        "model_family": "mistral3",
        "model_families": [
          "mistral3"
        ],
        "model_type": "24.0B",
        "file_type": "F16",
        "architecture": "amd64",
        "os": "linux",
        "rootfs": {
          "type": "layers",
          "diff_ids": [
            "sha256:e77b96cf6204c5022350ddda84f65151309ad925db54535dfa8ffaa771131315",
            "sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8",
            "sha256:70a4dab5e1d14953cc95c7d4ee1003f05c2474a39cd07fef2f8975c776455d33",
            "sha256:a00920c28dfd776d75b999418d3736291dad8f7cede46bd8301cf64738460d0e"
          ]
        }
      },
      "context_length": null,
      "model_type": "24.0B",
      "quantization": null,
      "base_model": null
    }
  ],
  "annotations": {
    "ollama.input_type": "text"
  },
  "quality_score": {
    "fields_filled": 12,
    "total_possible": 19,
    "completeness": 0.63
  }
}